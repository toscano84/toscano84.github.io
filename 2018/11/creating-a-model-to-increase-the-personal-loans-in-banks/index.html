

  
    
  


  





  

<!DOCTYPE html>
<html lang="en-us">
  <head>
    
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Hugo 0.49.2 with theme Tranquilpeak 0.4.3-BETA">
    <title>Creating a Model to Predict if a Bank Customer accepts Personal Loans</title>
    <meta name="author" content="Hugo Toscano">
    <meta name="keywords" content="">

    <link rel="icon" href="../../../favicon.png">
    

    
    <meta name="description" content="In this post, we will fit a multiple logistic regression model to predict the probability of a bank customer accepting a personal loan based on multiple variables to be described later. Logistic regression is a supervised learning algorithm were the independent variable has a qualitative nature. In this case, corresponding to the acceptance or rejection of a personal loan. This tutorial will build multiple logistic regression models and assess them.">
    <meta property="og:description" content="In this post, we will fit a multiple logistic regression model to predict the probability of a bank customer accepting a personal loan based on multiple variables to be described later. Logistic regression is a supervised learning algorithm were the independent variable has a qualitative nature. In this case, corresponding to the acceptance or rejection of a personal loan. This tutorial will build multiple logistic regression models and assess them.">
    <meta property="og:type" content="blog">
    <meta property="og:title" content="Creating a Model to Predict if a Bank Customer accepts Personal Loans">
    <meta property="og:url" content="/2018/11/creating-a-model-to-increase-the-personal-loans-in-banks/">
    <meta property="og:site_name" content="Hugo Toscano">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Hugo Toscano">
    <meta name="twitter:description" content="In this post, we will fit a multiple logistic regression model to predict the probability of a bank customer accepting a personal loan based on multiple variables to be described later. Logistic regression is a supervised learning algorithm were the independent variable has a qualitative nature. In this case, corresponding to the acceptance or rejection of a personal loan. This tutorial will build multiple logistic regression models and assess them.">
    
    

    
    

    
      <meta property="og:image" content="https://res.cloudinary.com/dabczf7ag/image/upload/v1528704790/foto1.jpg">
    

    
    
    

    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css" integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css" integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin="anonymous" />
    
    
    <link rel="stylesheet" href="../../../css/style-jsjn0006wyhpyzivf6yceb31gvpjatbcs3qzjvlumobfnugccvobqwxnnaj8.min.css" />
    
    

    
      
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-123-45', 'auto');
	
	ga('send', 'pageview');
}
</script>

    
    
  </head>

  <body>
    <div id="blog">
      <header id="header" data-behavior="4">
  <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
  <div class="header-title">
    <a class="header-title-link" href="../../../">Hugo Toscano</a>
  </div>
  
    
      <a class="header-right-picture "
         href="../../../#about">
    
    
    
      
        <img class="header-picture" src="https://res.cloudinary.com/dabczf7ag/image/upload/v1528704790/foto1.jpg" alt="Author&#39;s picture" />
      
    
    </a>
  
</header>

      <nav id="sidebar" data-behavior="4">
  <div class="sidebar-container">
    
      <div class="sidebar-profile">
        <a href="../../../#about">
          <img class="sidebar-profile-picture" src="https://res.cloudinary.com/dabczf7ag/image/upload/v1528704790/foto1.jpg" alt="Author&#39;s picture" />
        </a>
        <h4 class="sidebar-profile-name">Hugo Toscano</h4>
        
          <h5 class="sidebar-profile-bio">Contact: hugo_toscano@outlook.com</h5>
        
      </div>
    
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="../../../">
    
      <i class="sidebar-button-icon fa fa-lg fa-home"></i>
      
      <span class="sidebar-button-desc">Home</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="../../../2018/07/about-me/">
    
      <i class="sidebar-button-icon fa fa-lg fa-user"></i>
      
      <span class="sidebar-button-desc">About me</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="../../../categories">
    
      <i class="sidebar-button-icon fa fa-lg fa-bookmark"></i>
      
      <span class="sidebar-button-desc">Categories</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="../../../tags">
    
      <i class="sidebar-button-icon fa fa-lg fa-tags"></i>
      
      <span class="sidebar-button-desc">Tags</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="../../../archives">
    
      <i class="sidebar-button-icon fa fa-lg fa-search"></i>
      
      <span class="sidebar-button-desc">Archives</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://github.com/toscano84" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-github"></i>
      
      <span class="sidebar-button-desc">GitHub</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://www.linkedin.com/in/hugo-toscano-56766a2a/" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-linkedin"></i>
      
      <span class="sidebar-button-desc">LinkedIn</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://twitter.com/htoscano84" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-twitter"></i>
      
      <span class="sidebar-button-desc">Twitter</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      

    </ul>
  </div>
</nav>

      

      <div id="main" data-behavior="4"
        class="
               hasCoverMetaIn
               ">
        <article class="post" itemscope itemType="http://schema.org/BlogPosting">
          
          
            <div class="post-header main-content-wrap text-left">
  
    <h1 class="post-title" itemprop="headline">
      Creating a Model to Predict if a Bank Customer accepts Personal Loans
    </h1>
  
  
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2018-11-30T00:00:00Z">
        
  November 30, 2018

      </time>
    
    
  
  
    <span>in</span>
    
      <a class="category-link" href="../../../categories/predictive-analytics">Predictive Analytics</a>
    
  

  </div>

</div>
          
          <div class="post-content markdown" itemprop="articleBody">
            <div class="main-content-wrap">
              <p>In this post, we will fit a multiple logistic regression model to predict the probability of a bank customer accepting a personal loan based on multiple variables to be described later. Logistic regression is a supervised learning algorithm were the independent variable has a qualitative nature. In this case, corresponding to the acceptance or rejection of a personal loan. This tutorial will build multiple logistic regression models and assess them.</p>
<p>The data called UniversalBank comes again from the handbook ‘Data Mining for Business Analytics: Concepts, Techniques, and Applications in R’. The bank’s business goal is to find the best combination of variables that can increase the probability of loan acceptance.</p>
<div id="data-exploration" class="section level2">
<h2>Data Exploration</h2>
<p>First, we must load our libraries.</p>
<pre class="r"><code>library(here)
library(tidyverse) # data wrangling, data visualization
library(broom) # tidy statistics
library(caret) # apply machine learning algorithms
library(janitor) # tidy dataframes
library(MASS) # in this case it&#39;s used for the stepwise regression
library(readxl) # open excel files

options(scipen = 999) # number formatting to not include scientific notation</code></pre>
<p>Afterwards, we load our data frame and explore it. We also need to transform into factors a couple of our variables so that they are interpretable by our logistic regression models.</p>
<pre class="r"><code># open dataset
bank &lt;- read_excel(here::here(&quot;UniversalBank.xlsx&quot;), 
                   skip = 3,
                   sheet = 2) %&gt;%
  clean_names() %&gt;%
  mutate_at(vars(education, personal_loan, securities_account, cd_account,
                 online, credit_card), funs(as.factor))

glimpse(bank)</code></pre>
<pre><code>## Observations: 5,000
## Variables: 14
## $ id                 &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, ...
## $ age                &lt;dbl&gt; 25, 45, 39, 35, 35, 37, 53, 50, 35, 34, 65,...
## $ experience         &lt;dbl&gt; 1, 19, 15, 9, 8, 13, 27, 24, 10, 9, 39, 5, ...
## $ income             &lt;dbl&gt; 49, 34, 11, 100, 45, 29, 72, 22, 81, 180, 1...
## $ zip_code           &lt;dbl&gt; 91107, 90089, 94720, 94112, 91330, 92121, 9...
## $ family             &lt;dbl&gt; 4, 3, 1, 1, 4, 4, 2, 1, 3, 1, 4, 3, 2, 4, 1...
## $ cc_avg             &lt;dbl&gt; 1.6, 1.5, 1.0, 2.7, 1.0, 0.4, 1.5, 0.3, 0.6...
## $ education          &lt;fct&gt; 1, 1, 1, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 1...
## $ mortgage           &lt;dbl&gt; 0, 0, 0, 0, 0, 155, 0, 0, 104, 0, 0, 0, 0, ...
## $ personal_loan      &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0...
## $ securities_account &lt;fct&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1...
## $ cd_account         &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
## $ online             &lt;fct&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0...
## $ credit_card        &lt;fct&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0...</code></pre>
<p>Here we can see in more detail each variable description:</p>
<div class="figure">
<img src="../../../post/2018-10-23-predicting-airfares-prices-on-new-routes-a-supervised-learning-approach-with-multiple-linear-regression_files/variables_bank.PNG" style="width:80.0%" />

</div>
<p>Let’s continue our data exploration. We can summarize our data:</p>
<pre class="r"><code># explore dataset
summary(bank)</code></pre>
<pre><code>##        id            age          experience       income      
##  Min.   :   1   Min.   :23.00   Min.   :-3.0   Min.   :  8.00  
##  1st Qu.:1251   1st Qu.:35.00   1st Qu.:10.0   1st Qu.: 39.00  
##  Median :2500   Median :45.00   Median :20.0   Median : 64.00  
##  Mean   :2500   Mean   :45.34   Mean   :20.1   Mean   : 73.77  
##  3rd Qu.:3750   3rd Qu.:55.00   3rd Qu.:30.0   3rd Qu.: 98.00  
##  Max.   :5000   Max.   :67.00   Max.   :43.0   Max.   :224.00  
##     zip_code         family          cc_avg       education
##  Min.   : 9307   Min.   :1.000   Min.   : 0.000   1:2096   
##  1st Qu.:91911   1st Qu.:1.000   1st Qu.: 0.700   2:1403   
##  Median :93437   Median :2.000   Median : 1.500   3:1501   
##  Mean   :93153   Mean   :2.396   Mean   : 1.938            
##  3rd Qu.:94608   3rd Qu.:3.000   3rd Qu.: 2.500            
##  Max.   :96651   Max.   :4.000   Max.   :10.000            
##     mortgage     personal_loan securities_account cd_account online  
##  Min.   :  0.0   0:4520        0:4478             0:4698     0:2016  
##  1st Qu.:  0.0   1: 480        1: 522             1: 302     1:2984  
##  Median :  0.0                                                       
##  Mean   : 56.5                                                       
##  3rd Qu.:101.0                                                       
##  Max.   :635.0                                                       
##  credit_card
##  0:3530     
##  1:1470     
##             
##             
##             
## </code></pre>
<p>It’s also important to check for any missing values.</p>
<pre class="r"><code># check missing values
sum(is.na(bank))</code></pre>
<pre><code>## [1] 0</code></pre>
<p>Now that we have confirmed no values are missing, we can start with the visual exploration:</p>
<pre class="r"><code># boxplot

bank %&gt;%
  ggplot(aes(x = personal_loan, y = age, colour = personal_loan)) +
  geom_boxplot()</code></pre>
<p><img src="../../../post/2018-11-28-creating-a-model-to-increase-the-personal-loans-in-banks_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code># histogram
bank %&gt;%
  ggplot(aes(income, fill = personal_loan)) +
  geom_histogram(colour = &quot;black&quot;)</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="../../../post/2018-11-28-creating-a-model-to-increase-the-personal-loans-in-banks_files/figure-html/unnamed-chunk-5-2.png" width="672" /></p>
<p>It looks like the median age of persons who accepted the loan is similar to the median age of persons who did not accept it. However, the histogram shows that much less customers accepted the loan compared to the ones who rejected it.</p>
<p>This class imbalance, that is, many more people whose loan was rejected than accepted can be an issue to be taken into account when building our model. This post will focus on it later on .</p>
<p>For now, we can compute this imbalance in a more concrete way.</p>
<pre class="r"><code>bank %&gt;%
  count(personal_loan) %&gt;%
  mutate(prop = n / sum(n))</code></pre>
<pre><code>## # A tibble: 2 x 3
##   personal_loan     n  prop
##   &lt;fct&gt;         &lt;int&gt; &lt;dbl&gt;
## 1 0              4520 0.904
## 2 1               480 0.096</code></pre>
<p>Thus, only around 10% of our customers accepted the loan.</p>
</div>
<div id="building-logistic-regression-models" class="section level2">
<h2>Building Logistic Regression Models</h2>
<p>In this section, <strong>we will build our train and test datasets</strong>. The goal is to fit our training model, apply it in a testing data frame for validation and to see if it generalizes to new data. In order to create a train and test dataset, we can use the <code>createDataPartition()</code> function from the <code>caret</code> package to make this partition. But before that we should tidy our data frame.</p>
<pre class="r"><code>bank_tidy &lt;- bank %&gt;%
  dplyr::select(-id, -zip_code) # delete columns id and zipcode - not relevant for the logistic regression model

glimpse(bank) # overview of the dataframe</code></pre>
<pre><code>## Observations: 5,000
## Variables: 14
## $ id                 &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, ...
## $ age                &lt;dbl&gt; 25, 45, 39, 35, 35, 37, 53, 50, 35, 34, 65,...
## $ experience         &lt;dbl&gt; 1, 19, 15, 9, 8, 13, 27, 24, 10, 9, 39, 5, ...
## $ income             &lt;dbl&gt; 49, 34, 11, 100, 45, 29, 72, 22, 81, 180, 1...
## $ zip_code           &lt;dbl&gt; 91107, 90089, 94720, 94112, 91330, 92121, 9...
## $ family             &lt;dbl&gt; 4, 3, 1, 1, 4, 4, 2, 1, 3, 1, 4, 3, 2, 4, 1...
## $ cc_avg             &lt;dbl&gt; 1.6, 1.5, 1.0, 2.7, 1.0, 0.4, 1.5, 0.3, 0.6...
## $ education          &lt;fct&gt; 1, 1, 1, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 2, 1...
## $ mortgage           &lt;dbl&gt; 0, 0, 0, 0, 0, 155, 0, 0, 104, 0, 0, 0, 0, ...
## $ personal_loan      &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0...
## $ securities_account &lt;fct&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1...
## $ cd_account         &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...
## $ online             &lt;fct&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0...
## $ credit_card        &lt;fct&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0...</code></pre>
<pre class="r"><code>bank_tidy$personal_loan &lt;- factor(bank_tidy$personal_loan, levels=c(0,1), labels=c(&quot;No&quot;,&quot;Yes&quot;))  # label our dependent variable </code></pre>
<p>At this time we create a train and test datasets:</p>
<pre class="r"><code>set.seed(1234)
partition &lt;- createDataPartition(bank_tidy$personal_loan, 
                                 p = 0.7, 
                                 list = FALSE)

train &lt;- bank_tidy[partition, ]
test &lt;- bank_tidy[-partition, ]</code></pre>
<p>Now, we will create our first logistic regression model called <strong>model0</strong>:</p>
<pre class="r"><code>model0 &lt;- glm(personal_loan ~., data = train,
              family = &quot;binomial&quot;)

summary(model0)</code></pre>
<pre><code>## 
## Call:
## glm(formula = personal_loan ~ ., family = &quot;binomial&quot;, data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -3.0710  -0.1969  -0.0750  -0.0243   3.8229  
## 
## Coefficients:
##                        Estimate  Std. Error z value             Pr(&gt;|z|)
## (Intercept)         -11.6886570   2.1951529  -5.325    0.000000101087806
## age                  -0.0450697   0.0817190  -0.552             0.581277
## experience            0.0551820   0.0810369   0.681             0.495904
## income                0.0567793   0.0033629  16.884 &lt; 0.0000000000000002
## family                0.6535239   0.0907246   7.203    0.000000000000587
## cc_avg                0.2176102   0.0514232   4.232    0.000023187946205
## education2            3.6701132   0.3076904  11.928 &lt; 0.0000000000000002
## education3            3.6990124   0.3052904  12.116 &lt; 0.0000000000000002
## mortgage              0.0006775   0.0007203   0.941             0.346916
## securities_account1  -0.8186157   0.3396475  -2.410             0.015944
## cd_account1           3.6962953   0.4023738   9.186 &lt; 0.0000000000000002
## online1              -0.7498566   0.1940857  -3.864             0.000112
## credit_card1         -1.1885202   0.2605629  -4.561    0.000005082427535
##                        
## (Intercept)         ***
## age                    
## experience             
## income              ***
## family              ***
## cc_avg              ***
## education2          ***
## education3          ***
## mortgage               
## securities_account1 *  
## cd_account1         ***
## online1             ***
## credit_card1        ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2213.43  on 3499  degrees of freedom
## Residual deviance:  847.63  on 3487  degrees of freedom
## AIC: 873.63
## 
## Number of Fisher Scoring iterations: 8</code></pre>
<p>The model shows that all variables predict a bank costumer loan acceptance, but age, professional experience, and mortgage value. The coefficient estimates show a relationship between the predictors and the dependent variable on a log-odds scale. For instance, an increase of one person in a family is associated with an increase in the log odds of a personal loan acceptance by 0.654 units.</p>
<p>This can be interpreted in a more concrete manner by computing the odds.</p>
<pre class="r"><code>exp(coef(model0))</code></pre>
<pre><code>##         (Intercept)                 age          experience 
##      0.000008388432      0.955930861623      1.056732872249 
##              income              family              cc_avg 
##      1.058422211865      1.922302973299      1.243102411023 
##          education2          education3            mortgage 
##     39.256351249957     40.407379004231      1.000677707014 
## securities_account1         cd_account1             online1 
##      0.441041769988     40.297738262092      0.472434286781 
##        credit_card1 
##      0.304671776448</code></pre>
<p>This shows that an increase of one family member, increases the odds of a bank costumer accepting a personal loan by a factor of 1.922.</p>
<p>Let’s now check if we have outliers in our data set. In case we find them , we will not delete them; it’s just something to have in mind while doing this type of analysis. <strong>Note: we will do this analysis only for model0.</strong> First, we should use the <code>augment()</code> function from the <code>broom</code> package to get the residuals data of our statistical model. In the next step, we should use <code>ggplot2</code> to visualize possible outliers.</p>
<pre class="r"><code>#- check for some assumptions
# Residual Assessment
tidy_mod0 &lt;- broom::augment(model0) %&gt;%
  mutate(index = 1:n())

tidy_mod0 %&gt;%
  ggplot(aes(x = index, y = .std.resid, colour = personal_loan)) +
  geom_point(alpha = 0.4)</code></pre>
<p><img src="../../../post/2018-11-28-creating-a-model-to-increase-the-personal-loans-in-banks_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>It looks like we have some cases above 3. Let’s check how many there are and the corresponding number of the case.</p>
<pre class="r"><code>tidy_mod0 %&gt;%
  filter(abs(.std.resid) &gt; 3) %&gt;%
  count()</code></pre>
<pre><code>## # A tibble: 1 x 1
##       n
##   &lt;int&gt;
## 1     8</code></pre>
<pre class="r"><code>plot(model0, which = 4, id.n = 8)</code></pre>
<p><img src="../../../post/2018-11-28-creating-a-model-to-increase-the-personal-loans-in-banks_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>In total, we have 8 cases exceeding 3 and in the graph above we can see the number of the corresponding case.</p>
<p>Now, we should move on and focus in fitting our training data. For the creation of model1, we will use a stepwise regression with the function <code>stepAIC()</code> from the <code>MASS</code> package.</p>
<pre class="r"><code># model1
step_log &lt;- stepAIC(model0, direction = &quot;both&quot;)</code></pre>
<pre><code>## Start:  AIC=873.63
## personal_loan ~ age + experience + income + family + cc_avg + 
##     education + mortgage + securities_account + cd_account + 
##     online + credit_card
## 
##                      Df Deviance     AIC
## - age                 1   847.94  871.94
## - experience          1   848.10  872.10
## - mortgage            1   848.51  872.51
## &lt;none&gt;                    847.63  873.63
## - securities_account  1   853.99  877.99
## - online              1   862.82  886.82
## - cc_avg              1   866.06  890.06
## - credit_card         1   871.61  895.61
## - family              1   905.35  929.35
## - cd_account          1   948.44  972.44
## - education           2  1110.12 1132.12
## - income              1  1353.89 1377.89
## 
## Step:  AIC=871.94
## personal_loan ~ experience + income + family + cc_avg + education + 
##     mortgage + securities_account + cd_account + online + credit_card
## 
##                      Df Deviance     AIC
## - mortgage            1   848.82  870.82
## - experience          1   849.79  871.79
## &lt;none&gt;                    847.94  871.94
## + age                 1   847.63  873.63
## - securities_account  1   854.20  876.20
## - online              1   863.10  885.10
## - cc_avg              1   866.47  888.47
## - credit_card         1   871.87  893.87
## - family              1   905.49  927.49
## - cd_account          1   949.00  971.00
## - education           2  1113.95 1133.95
## - income              1  1362.70 1384.70
## 
## Step:  AIC=870.82
## personal_loan ~ experience + income + family + cc_avg + education + 
##     securities_account + cd_account + online + credit_card
## 
##                      Df Deviance     AIC
## - experience          1   850.63  870.63
## &lt;none&gt;                    848.82  870.82
## + mortgage            1   847.94  871.94
## + age                 1   848.51  872.51
## - securities_account  1   855.11  875.11
## - online              1   863.90  883.90
## - cc_avg              1   866.69  886.69
## - credit_card         1   873.04  893.04
## - family              1   906.55  926.55
## - cd_account          1   950.10  970.10
## - education           2  1114.58 1132.58
## - income              1  1375.72 1395.72
## 
## Step:  AIC=870.63
## personal_loan ~ income + family + cc_avg + education + securities_account + 
##     cd_account + online + credit_card
## 
##                      Df Deviance     AIC
## &lt;none&gt;                    850.63  870.63
## + experience          1   848.82  870.82
## + age                 1   848.98  870.98
## + mortgage            1   849.79  871.79
## - securities_account  1   856.79  874.79
## - online              1   865.53  883.53
## - cc_avg              1   867.64  885.64
## - credit_card         1   874.19  892.19
## - family              1   907.85  925.85
## - cd_account          1   951.40  969.40
## - education           2  1115.49 1131.49
## - income              1  1376.23 1394.23</code></pre>
<p>We can see which variables were kept for our model.</p>
<pre class="r"><code>step_log</code></pre>
<pre><code>## 
## Call:  glm(formula = personal_loan ~ income + family + cc_avg + education + 
##     securities_account + cd_account + online + credit_card, family = &quot;binomial&quot;, 
##     data = train)
## 
## Coefficients:
##         (Intercept)               income               family  
##           -12.53157              0.05694              0.65103  
##              cc_avg           education2           education3  
##             0.20736              3.63049              3.62850  
## securities_account1          cd_account1              online1  
##            -0.80475              3.69255             -0.74104  
##        credit_card1  
##            -1.17166  
## 
## Degrees of Freedom: 3499 Total (i.e. Null);  3490 Residual
## Null Deviance:       2213 
## Residual Deviance: 850.6     AIC: 870.6</code></pre>
<p>Now, it’s time to create our model1.</p>
<pre class="r"><code>model1 &lt;- glm(personal_loan ~ income + family + cc_avg + education + 
                securities_account + cd_account + online + credit_card, family = &quot;binomial&quot;, 
              data = train)

summary(model1)</code></pre>
<pre><code>## 
## Call:
## glm(formula = personal_loan ~ income + family + cc_avg + education + 
##     securities_account + cd_account + online + credit_card, family = &quot;binomial&quot;, 
##     data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -3.0644  -0.2006  -0.0757  -0.0248   3.8089  
## 
## Coefficients:
##                       Estimate Std. Error z value             Pr(&gt;|z|)    
## (Intercept)         -12.531566   0.649065 -19.307 &lt; 0.0000000000000002 ***
## income                0.056935   0.003338  17.057 &lt; 0.0000000000000002 ***
## family                0.651028   0.090765   7.173    0.000000000000735 ***
## cc_avg                0.207358   0.051010   4.065    0.000048021236068 ***
## education2            3.630494   0.305323  11.891 &lt; 0.0000000000000002 ***
## education3            3.628497   0.299617  12.110 &lt; 0.0000000000000002 ***
## securities_account1  -0.804754   0.338964  -2.374             0.017589 *  
## cd_account1           3.692554   0.401937   9.187 &lt; 0.0000000000000002 ***
## online1              -0.741044   0.193565  -3.828             0.000129 ***
## credit_card1         -1.171660   0.258974  -4.524    0.000006061455975 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2213.43  on 3499  degrees of freedom
## Residual deviance:  850.63  on 3490  degrees of freedom
## AIC: 870.63
## 
## Number of Fisher Scoring iterations: 8</code></pre>
<p>All variables predict the loan acceptance. We will create one last model, called model2. The most important predictors present in model1 will be applied to this last model. To get the information about the most relevant predictors, we will use the <code>varImp()</code> from the <code>caret</code> package.</p>
<pre class="r"><code>caret::varImp(model1) %&gt;%
  tibble::rownames_to_column(&quot;variable&quot;) %&gt;%
  arrange(desc(Overall))</code></pre>
<pre><code>##              variable   Overall
## 1              income 17.057007
## 2          education3 12.110460
## 3          education2 11.890650
## 4         cd_account1  9.186890
## 5              family  7.172696
## 6        credit_card1  4.524234
## 7              cc_avg  4.065055
## 8             online1  3.828393
## 9 securities_account1  2.374156</code></pre>
<p>We will choose some of the most important variables and create model2.</p>
<pre class="r"><code># model2
model2 &lt;- glm(personal_loan ~ income + family + education + cd_account + credit_card + cc_avg + 
                online, family = &quot;binomial&quot;, 
              data = train)

summary(model2)</code></pre>
<pre><code>## 
## Call:
## glm(formula = personal_loan ~ income + family + education + cd_account + 
##     credit_card + cc_avg + online, family = &quot;binomial&quot;, data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.9380  -0.2015  -0.0754  -0.0249   3.8286  
## 
## Coefficients:
##               Estimate Std. Error z value             Pr(&gt;|z|)    
## (Intercept)  -12.67077    0.64658 -19.596 &lt; 0.0000000000000002 ***
## income         0.05731    0.00333  17.211 &lt; 0.0000000000000002 ***
## family         0.65229    0.09047   7.210    0.000000000000559 ***
## education2     3.65868    0.30516  11.989 &lt; 0.0000000000000002 ***
## education3     3.62688    0.29823  12.161 &lt; 0.0000000000000002 ***
## cd_account1    3.23195    0.34597   9.342 &lt; 0.0000000000000002 ***
## credit_card1  -1.06774    0.25258  -4.227    0.000023642291067 ***
## cc_avg         0.20572    0.05079   4.051    0.000051093604328 ***
## online1       -0.70415    0.19220  -3.664             0.000249 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2213.43  on 3499  degrees of freedom
## Residual deviance:  856.79  on 3491  degrees of freedom
## AIC: 874.79
## 
## Number of Fisher Scoring iterations: 8</code></pre>
</div>
<div id="assssment-of-the-logistic-regression-models" class="section level2">
<h2>Assssment of the Logistic Regression Models</h2>
<p>Now with the help of the <code>caret</code> package we have to assess which model has a better performance. We will use three key performance metrics: <strong>accuracy</strong>, <strong>ppv</strong>, or positive predicted values, and <strong>npv</strong>, negative predicted values. Accuracy corresponds to the True Positives (TP) + the True Negatives(TN) divided by the TP + TN + False Positive(FP) + False Negatives(FN). PPV corresponds to the cases rightfully identified as positive(TP) divided by the TP + FP. The NPV is the number of cases rightfully identified as negative(TN) divided by the TN + FN.</p>
<p>It’s now time to build our models by using the function <code>confusionMatrix</code> to compute their metrics. <strong>Note</strong>: For each model, a sampling method called upsampling will be used due to the imbalance present in our dependent variable.</p>
<ul>
<li><strong>model0</strong></li>
</ul>
<pre class="r"><code># model0
glm0 &lt;- train(personal_loan ~., method = &quot;glm&quot;,
              family = &quot;binomial&quot;,
              data = train,
              trControl = trainControl(method = &quot;none&quot;, 
                                       sampling = &quot;up&quot;)) # upsampling use because 

confusionMatrix(predict(glm0, 
                        train), train$personal_loan, positive = &quot;Yes&quot;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   No  Yes
##        No  2843   38
##        Yes  321  298
##                                              
##                Accuracy : 0.8974             
##                  95% CI : (0.8869, 0.9073)   
##     No Information Rate : 0.904              
##     P-Value [Acc &gt; NIR] : 0.9103             
##                                              
##                   Kappa : 0.5707             
##  Mcnemar&#39;s Test P-Value : &lt;0.0000000000000002
##                                              
##             Sensitivity : 0.88690            
##             Specificity : 0.89855            
##          Pos Pred Value : 0.48142            
##          Neg Pred Value : 0.98681            
##              Prevalence : 0.09600            
##          Detection Rate : 0.08514            
##    Detection Prevalence : 0.17686            
##       Balanced Accuracy : 0.89273            
##                                              
##        &#39;Positive&#39; Class : Yes                
## </code></pre>
<ul>
<li><strong>model1</strong></li>
</ul>
<pre class="r"><code>glm1 &lt;- train(personal_loan ~ income + family + cc_avg + education + 
    securities_account + cd_account + online + credit_card, method = &quot;glm&quot;, 
              family = &quot;binomial&quot;,
              data = train,
              trControl = trainControl(method = &quot;none&quot;, 
                                       sampling = &quot;up&quot;))

confusionMatrix(predict(glm1, 
                        train), train$personal_loan, positive = &quot;Yes&quot;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   No  Yes
##        No  2843   39
##        Yes  321  297
##                                              
##                Accuracy : 0.8971             
##                  95% CI : (0.8866, 0.907)    
##     No Information Rate : 0.904              
##     P-Value [Acc &gt; NIR] : 0.919              
##                                              
##                   Kappa : 0.569              
##  Mcnemar&#39;s Test P-Value : &lt;0.0000000000000002
##                                              
##             Sensitivity : 0.88393            
##             Specificity : 0.89855            
##          Pos Pred Value : 0.48058            
##          Neg Pred Value : 0.98647            
##              Prevalence : 0.09600            
##          Detection Rate : 0.08486            
##    Detection Prevalence : 0.17657            
##       Balanced Accuracy : 0.89124            
##                                              
##        &#39;Positive&#39; Class : Yes                
## </code></pre>
<ul>
<li><strong>model2</strong></li>
</ul>
<pre class="r"><code>glm2 &lt;- train(personal_loan ~ income + family + education + cd_account + 
    credit_card + cc_avg + online, method = &quot;glm&quot;, 
              family = &quot;binomial&quot;,
              data = train,
              trControl = trainControl(method = &quot;none&quot;, 
                                       sampling = &quot;up&quot;))

confusionMatrix(predict(glm2, 
                        train), train$personal_loan, positive = &quot;Yes&quot;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   No  Yes
##        No  2852   37
##        Yes  312  299
##                                              
##                Accuracy : 0.9003             
##                  95% CI : (0.8899, 0.91)     
##     No Information Rate : 0.904              
##     P-Value [Acc &gt; NIR] : 0.7816             
##                                              
##                   Kappa : 0.5794             
##  Mcnemar&#39;s Test P-Value : &lt;0.0000000000000002
##                                              
##             Sensitivity : 0.88988            
##             Specificity : 0.90139            
##          Pos Pred Value : 0.48936            
##          Neg Pred Value : 0.98719            
##              Prevalence : 0.09600            
##          Detection Rate : 0.08543            
##    Detection Prevalence : 0.17457            
##       Balanced Accuracy : 0.89564            
##                                              
##        &#39;Positive&#39; Class : Yes                
## </code></pre>
<p>Looking at the 3 confusion matrices, it seems that model2 has a higher accuracy, ppv, and npv. Nonetheless, model0 and model1 are also highly accurate. However, we still have to check how these models generalize to new data.</p>
</div>
<div id="generalization-of-the-logistic-regression-models-to-new-data" class="section level2">
<h2>Generalization of the Logistic Regression Models to new Data</h2>
<p>Now, we should check how the models generalize to new data.</p>
<ul>
<li><strong>model0</strong></li>
</ul>
<pre class="r"><code># model0
confusionMatrix(predict(glm0, 
                        test), test$personal_loan, positive = &quot;Yes&quot;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   No  Yes
##        No  1227    9
##        Yes  129  135
##                                              
##                Accuracy : 0.908              
##                  95% CI : (0.8922, 0.9221)   
##     No Information Rate : 0.904              
##     P-Value [Acc &gt; NIR] : 0.3181             
##                                              
##                   Kappa : 0.6138             
##  Mcnemar&#39;s Test P-Value : &lt;0.0000000000000002
##                                              
##             Sensitivity : 0.9375             
##             Specificity : 0.9049             
##          Pos Pred Value : 0.5114             
##          Neg Pred Value : 0.9927             
##              Prevalence : 0.0960             
##          Detection Rate : 0.0900             
##    Detection Prevalence : 0.1760             
##       Balanced Accuracy : 0.9212             
##                                              
##        &#39;Positive&#39; Class : Yes                
## </code></pre>
<ul>
<li><strong>model1</strong></li>
</ul>
<pre class="r"><code># model1
confusionMatrix(predict(glm1, 
                        test), test$personal_loan, positive = &quot;Yes&quot;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   No  Yes
##        No  1226    9
##        Yes  130  135
##                                              
##                Accuracy : 0.9073             
##                  95% CI : (0.8915, 0.9215)   
##     No Information Rate : 0.904              
##     P-Value [Acc &gt; NIR] : 0.3503             
##                                              
##                   Kappa : 0.6119             
##  Mcnemar&#39;s Test P-Value : &lt;0.0000000000000002
##                                              
##             Sensitivity : 0.9375             
##             Specificity : 0.9041             
##          Pos Pred Value : 0.5094             
##          Neg Pred Value : 0.9927             
##              Prevalence : 0.0960             
##          Detection Rate : 0.0900             
##    Detection Prevalence : 0.1767             
##       Balanced Accuracy : 0.9208             
##                                              
##        &#39;Positive&#39; Class : Yes                
## </code></pre>
<ul>
<li><strong>model2</strong></li>
</ul>
<pre class="r"><code># model2
confusionMatrix(predict(glm2, 
                        test), test$personal_loan, positive = &quot;Yes&quot;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   No  Yes
##        No  1226   11
##        Yes  130  133
##                                              
##                Accuracy : 0.906              
##                  95% CI : (0.8901, 0.9203)   
##     No Information Rate : 0.904              
##     P-Value [Acc &gt; NIR] : 0.4177             
##                                              
##                   Kappa : 0.6045             
##  Mcnemar&#39;s Test P-Value : &lt;0.0000000000000002
##                                              
##             Sensitivity : 0.92361            
##             Specificity : 0.90413            
##          Pos Pred Value : 0.50570            
##          Neg Pred Value : 0.99111            
##              Prevalence : 0.09600            
##          Detection Rate : 0.08867            
##    Detection Prevalence : 0.17533            
##       Balanced Accuracy : 0.91387            
##                                              
##        &#39;Positive&#39; Class : Yes                
## </code></pre>
<p>All models maintain or slightly increase their metrics in the test dataset. Thus, the metrics of all models are good.</p>
<p>Nonetheless, to keep our model simpler, we will choose model2. Now that we have our final model, it’s time for some predictions. Let us imagine that we have two bank customers. They have the same characteristics in relation to the variables of our model2, but one, income. While customer A has an annual income of 100 thousand dollars, customer B has a 45 thousand dollars annual income. We can use the <code>predict()</code> function to compare the probability of accepting a loan based on this income difference.</p>
<pre class="r"><code>predict(model2, data.frame(income = c(100, 45),
        family = c(3, 3),
        cc_avg = c(0.8, 0.8),
        education = c(&quot;3&quot;, &quot;3&quot;),
        securities_account = c(&quot;1&quot;, &quot;1&quot;),
        cd_account = c(&quot;1&quot;, &quot;1&quot;),
        credit_card = c(&quot;1&quot;, &quot;1&quot;),
        online = c(&quot;1&quot;, &quot;1&quot;)), 
type = &quot;response&quot;)</code></pre>
<pre><code>##          1          2 
## 0.56672287 0.05297574</code></pre>
<p>As a result, the probability of accepting a loan is of 56.67% for customer A, while for customer B is only 5.30%.</p>
<p>As mentioned in a previous post, a more advanced algorithm could have given us more predictive power. So, we should always keep that in mind. Hope you liked this post. Thanks again and feel free to contact me!</p>
</div>

              
            </div>
          </div>
          <div id="post-footer" class="post-footer main-content-wrap">
            
              
                
                
                  <div class="post-footer-tags">
                    <span class="text-color-light text-small">TAGGED IN</span><br/>
                    
  <a class="tag tag--primary tag--small" href="../../../tags/machine-learning/">machine learning</a>

  <a class="tag tag--primary tag--small" href="../../../tags/logistic-regression/">logistic regression</a>

  <a class="tag tag--primary tag--small" href="../../../tags/predictive-analytics/">predictive analytics</a>

                  </div>
                
              
            
            
  <div class="post-actions-wrap">
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="../../../2018/12/text-mining-crime-and-punishment-and-anna-karenina-a-tidytext-approach/" data-tooltip="Text Mining Crime and Punishment &amp; Anna Karenina: A Tidytext Approach">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">NEXT</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="../../../2018/11/german-elections-in-the-21st-century/" data-tooltip="German Elections in the 21st Century">
              
                  <span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://toscano84.github.io/2018/11/creating-a-model-to-increase-the-personal-loans-in-banks/">
              <i class="fa fa-facebook-official"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=https://toscano84.github.io/2018/11/creating-a-model-to-increase-the-personal-loans-in-banks/">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
        
      
      
        <li class="post-action">
          <a class="post-action-btn btn btn--default" href="#disqus_thread">
            <i class="fa fa-comment-o"></i>
          </a>
        </li>
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  </div>


            
              
                <div id="disqus_thread">
  <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
              
            
          </div>
        </article>
        <footer id="footer" class="main-content-wrap">
  <span class="copyrights">
    &copy; 2019 Hugo Toscano. All Rights Reserved
  </span>
</footer>

      </div>
      <div id="bottom-bar" class="post-bottom-bar" data-behavior="4">
        
  <div class="post-actions-wrap">
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="../../../2018/12/text-mining-crime-and-punishment-and-anna-karenina-a-tidytext-approach/" data-tooltip="Text Mining Crime and Punishment &amp; Anna Karenina: A Tidytext Approach">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">NEXT</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="../../../2018/11/german-elections-in-the-21st-century/" data-tooltip="German Elections in the 21st Century">
              
                  <span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://toscano84.github.io/2018/11/creating-a-model-to-increase-the-personal-loans-in-banks/">
              <i class="fa fa-facebook-official"></i>
            </a>
          </li>
        
          <li class="post-action hide-xs">
            <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=https://toscano84.github.io/2018/11/creating-a-model-to-increase-the-personal-loans-in-banks/">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
        
      
      
        <li class="post-action">
          <a class="post-action-btn btn btn--default" href="#disqus_thread">
            <i class="fa fa-comment-o"></i>
          </a>
        </li>
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  </div>


      </div>
      <div id="share-options-bar" class="share-options-bar" data-behavior="4">
  <i id="btn-close-shareoptions" class="fa fa-close"></i>
  <ul class="share-options">
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Ftoscano84.github.io%2F2018%2F11%2Fcreating-a-model-to-increase-the-personal-loans-in-banks%2F">
          <i class="fa fa-facebook-official"></i><span>Share on Facebook</span>
        </a>
      </li>
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://twitter.com/intent/tweet?text=https%3A%2F%2Ftoscano84.github.io%2F2018%2F11%2Fcreating-a-model-to-increase-the-personal-loans-in-banks%2F">
          <i class="fa fa-twitter"></i><span>Share on Twitter</span>
        </a>
      </li>
    
  </ul>
</div>
<div id="share-options-mask" class="share-options-mask"></div>
    </div>
    
    <div id="about">
  <div id="about-card">
    <div id="about-btn-close">
      <i class="fa fa-remove"></i>
    </div>
    
      <img id="about-card-picture" src="https://res.cloudinary.com/dabczf7ag/image/upload/v1528704790/foto1.jpg" alt="Author&#39;s picture" />
    
    <h4 id="about-card-name">Hugo Toscano</h4>
    
      <div id="about-card-bio">Contact: hugo_toscano@outlook.com</div>
    
    
    
      <div id="about-card-location">
        <i class="fa fa-map-marker"></i>
        <br/>
        Stuttgart, Germany
      </div>
    
  </div>
</div>

    <div id="algolia-search-modal" class="modal-container">
  <div class="modal">
    <div class="modal-header">
      <span class="close-button"><i class="fa fa-close"></i></span>
      <a href="https://algolia.com" target="_blank" rel="noopener" class="searchby-algolia text-color-light link-unstyled">
        <span class="searchby-algolia-text text-color-light text-small">by</span>
        <img class="searchby-algolia-logo" src="https://www.algolia.com/static_assets/images/press/downloads/algolia-light.svg">
      </a>
      <i class="search-icon fa fa-search"></i>
      <form id="algolia-search-form">
        <input type="text" id="algolia-search-input" name="search"
          class="form-control input--large search-input" placeholder="Search" />
      </form>
    </div>
    <div class="modal-body">
      <div class="no-result text-color-light text-center">no post found</div>
      <div class="results">
        
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://toscano84.github.io/2019/03/clustering-the-pharmaceutical-industry-stocks/">
                <h3 class="media-heading">Clustering the Pharmaceutical Industry Stocks</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Mar 3, 2019
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">In this post I will use two of the most popular clustering methods, hierarchical clustering and k-means clustering, to analyse a data frame related to the financial variables of some pharmaceutical companies. Clustering is an unsupervised learning technique where we segment the data and identify meaningful groups that have similar characteristics. In our case, the goal will be to find these groups within the pharmaceutical companies data. Like we did in the previous posts we will start by loading the required packages to our analysis.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://toscano84.github.io/2018/12/text-mining-crime-and-punishment-and-anna-karenina-a-tidytext-approach/">
                <h3 class="media-heading">Text Mining Crime and Punishment &amp; Anna Karenina: A Tidytext Approach</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Dec 12, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Welcome to a new exciting post! Today I have decided to bring you text mining applied to two of my favorite novels: Crime and Punishment by Dostoyevsky and Anna Karenina by Tolstoy. We will use mainly the incredible tidytext package developed by Julia Silge and David Robinson. You can read more about this package in the book of the same authors Text Mining with R: A Tidytext Approach.
Let us start the analysis of “Crime and Punishment” and “Anna Karenina” by loading the required packages.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://toscano84.github.io/2018/11/creating-a-model-to-increase-the-personal-loans-in-banks/">
                <h3 class="media-heading">Creating a Model to Predict if a Bank Customer accepts Personal Loans</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Nov 11, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">In this post, we will fit a multiple logistic regression model to predict the probability of a bank customer accepting a personal loan based on multiple variables to be described later. Logistic regression is a supervised learning algorithm were the independent variable has a qualitative nature. In this case, corresponding to the acceptance or rejection of a personal loan. This tutorial will build multiple logistic regression models and assess them.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://toscano84.github.io/2018/11/german-elections-in-the-21st-century/">
                <h3 class="media-heading">German Elections in the 21st Century</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Nov 11, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">In this blogpost, we will come back to the subject of the German Elections. We will try to show, mostly visually, the changes in election results during the 21st century. Thus, we will use data from the elections in 2002 to the last ones in 2017. The main focus will be mapping the results of the parties represented in the current Bundestag (German Parliament) during this time span. Let’s start our coding.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://toscano84.github.io/2018/10/predicting-airfares-on-new-routes/">
                <h3 class="media-heading">Predicting Airfares on New Routes a Supervised Learning Approach With Multiple Linear Regression</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Oct 10, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">This post will talk about multiple linear regression in the context of machine learning. Linear regression is one of the simplest and most used approaches for supervised learning. This tutorial will try to help you in how to use the linear regression algorithm. I am also new to the machine learning approach, but I’m very interested in this area given the predictive ability that you can gain from this. Let’s hope I can help you.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://toscano84.github.io/2018/10/hints-to-deal-with-missing-values-in-r/">
                <h3 class="media-heading">Hints to deal with Missing Values in R</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Oct 10, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">In R missing values are usually, but not always, represented by letters NA. How to deal with missing values is very important in the data analytics world. Missing data can be sometimes tricky while analyzing a data frame, since it should be handled correctly for our statistical analysis. Before diving into more complex details about missing data, the first question that should be asked in any exploratory data analysis is: Do I have missing values in my database?</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://toscano84.github.io/2018/09/combine-data-frames-in-r/">
                <h3 class="media-heading">Combine Data frames in R</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Sep 9, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Sometimes, before we start to explore our data, we need to put them together. For instance, we might have them stored in different data frames and we have to join variables from two or more data frames in one. This post will talk about the different functions we can use to achieve that goal. We will be using the dplyr package to combine different data frames.
Firstly, we will show examples related to what is called mutating joins.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://toscano84.github.io/2018/08/a-leaflet-approach-to-coffee-chains-starbucks-versus-dunkin-donuts/">
                <h3 class="media-heading">A Leaflet approach to Coffee Chains: Starbucks versus Dunkin&#39; Donuts</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Aug 8, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">This post talks about making interactive visualizations in R with leaflet(). In this example, I’ll map the USA locations of two of the biggest coffee chains, Starbucks and Dunkin’ Donuts. This package allows us to map data and play interactively with it. For instance, we can zoom in or zoom out to augment or diminish map details, respectively. We can add markers that signal the position of our data in the map and move the mouse cursor over to get information about it.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://toscano84.github.io/2018/07/linear-regression-in-r-a-simple-analysis/">
                <h3 class="media-heading">Tuition costs and gdp per capita: A linear regression analysis</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Jul 7, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">This post will explore with R one of the simplest approaches to predict a response of a quantitative nature. This approach is called Linear Regression.
I will use a simple Linear Regression to study whether there is any relationship between the gross domestic product (gdp) per capita of each state in the USA and its tuition costs. Therefore, our predictor will be the gdp per capita per state and our response will be the tuition costs per state.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="https://toscano84.github.io/2018/07/using-r-to-analyse-the-german-federal-election/">
                <h3 class="media-heading">Using R to analyse the German Federal Election</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Jul 7, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">As the title of this post implies we will analyze, using the statistical programming language R, the German Federal Election which took place on 24 September of 2017. It will not be an exhaustive analysis of the results. I’m only interested in visualizing the share of the vote that each party represented in the Parliament (i.e. Bundestag) received in each one of the 16 States of Germany.
In order to make this visualization possible in R, loading the respective packages is the first step.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
      </div>
    </div>
    <div class="modal-footer">
      <p class="results-count text-medium"
         data-message-zero="no post found"
         data-message-one="1 post found"
         data-message-other="{n} posts found">
         11 posts found
      </p>
    </div>
  </div>
</div>
    
  
    
    <div id="cover" style="background-image:url('https://res.cloudinary.com/dabczf7ag/image/upload/v1533024537/cover_new2.jpg');"></div>
  


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js" integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin="anonymous"></script>


<script src="../../../js/script-qi9wbxp2ya2j6p7wx1i6tgavftewndznf4v0hy2gvivk1rxgc3lm7njqb6bz.min.js"></script>



<script>
$(document).ready(function() {
  hljs.configure({ classPrefix: '', useBR: false });
  $('pre.code-highlight > code, pre > code').each(function(i, block) {
    if (!$(this).hasClass('codeblock')) {
      $(this).addClass('codeblock');
    }
    hljs.highlightBlock(block);
  });
});
</script>


  
    
      <script>
        var disqus_config = function () {
          this.page.url = 'https:\/\/toscano84.github.io\/2018\/11\/creating-a-model-to-increase-the-personal-loans-in-banks\/';
          
            this.page.identifier = '\/2018\/11\/creating-a-model-to-increase-the-personal-loans-in-banks\/'
          
        };
        (function() {
          
          
          if (window.location.hostname == "localhost") {
            return;
          }
          var d = document, s = d.createElement('script');
          var disqus_shortname = 'toscano84';
          s.src = '//' + disqus_shortname + '.disqus.com/embed.js';

          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
        })();
      </script>
    
  



    
  </body>
</html>

